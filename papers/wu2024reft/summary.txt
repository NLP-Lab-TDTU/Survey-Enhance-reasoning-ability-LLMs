 * Representation Finetuning (ReFT) is a new approach to adapting pretrained language models to new tasks, which involves learning task-specific interventions on hidden representations instead of updating model weights.
* The authors propose a specific instance of ReFT called Low-rank Linear Subspace ReFT (LoReFT), which is a drop-in replacement for existing parameter-efficient finetuning (PEFT) methods and learns interventions that are more parameter-efficient than prior state-of-the-art PEFTs.
* LoReFT is evaluated on eight commonsense reasoning tasks, four arithmetic reasoning tasks, Alpaca-Eval v1.0, and GLUE, and is shown to deliver the best balance of efficiency and performance, outperforming state-of-the-art PEFTs in most cases.
* The authors release a generic ReFT training library publicly at <https://github.com/stanfordnlp/pyreft>.
* ReFT methods offer a promising alternative to traditional PEFT methods, as they can leverage the rich semantic information encoded in representations to achieve better performance with fewer trainable parameters.