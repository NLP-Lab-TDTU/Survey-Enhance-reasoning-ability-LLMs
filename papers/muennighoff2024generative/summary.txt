 * The paper introduces a new method called Generative Representational Instruction Tuning (GRIT) that enables a large language model to handle both generative and embedding tasks by distinguishing between them through instructions.
* The resulting GRITLM 7B model sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks.
* GRITLM 8X7B outperforms all open generative language models while still being among the best embedding models.
* GRIT matches training on only generative or embedding data, thus unifying both at no performance loss.
* The unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by > 60% for long documents.
* The models, code, and other materials are available at <https://github.com/ContextualAI/gritlm>.