 * The paper introduces EURUS, a suite of large language models (LLMs) optimized for reasoning, which achieve state-of-the-art results on a diverse set of benchmarks covering mathematics, code generation, and logical reasoning problems.
* EURUS-70B beats GPT-3.5 Turbo in reasoning across five tasks and substantially outperforms existing open-source models on LeetCode and TheoremQA by margins more than 13.3%.
* The strong performance of EURUS can be primarily attributed to ULTRAINTERACT, a newly-curated large-scale, high-quality alignment dataset specifically designed for complex reasoning tasks.
* ULTRAINTERACT includes a preference tree consisting of reasoning chains, multi-turn interaction trajectories, and pairwise data to facilitate preference learning.
* The investigation reveals that some well-established preference learning algorithms may be less suitable for reasoning tasks, and a novel reward modeling objective is derived, leading to a strong reward model.