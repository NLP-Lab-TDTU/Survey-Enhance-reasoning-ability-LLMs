 * Large language models (LMs) trained on extensive datasets exhibit impressive capabilities but may also replicate undesirable behaviors from their human trainers.
* Current methods for aligning LMs with human preferences typically involve reinforcement learning (RL) to fine-tune the models, but these methods can be complex, unstable, and computationally expensive.
* The authors propose Direct Preference Optimization (DPO), a new parameterization of the reward model in RLHF that allows for closed-form extraction of the optimal policy, simplifying the process to a simple classification loss.
* DPO is stable, efficient, and eliminates the need for sampling from the LM during fine-tuning or hyperparameter tuning.
* Experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods, improving control over sentiment in generations and matching or improving response quality in summarization and single-turn dialogue.