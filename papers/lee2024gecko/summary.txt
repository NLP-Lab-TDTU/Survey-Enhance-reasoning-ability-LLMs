 * Gecko is a new text embedding model that is compact and versatile, achieving strong retrieval performance.
* It distills knowledge from large language models (LLMs) into a retriever using a two-step process.
* The first step generates diverse, synthetic paired data using an LLM, and the second step refines the data quality by retrieving a set of candidate passages for each query and relabeling the positive and hard negative passages using the same LLM.
* Gecko outperforms existing entries with 768 embedding size and competes with 7x larger models and 5x higher dimensional embeddings.
* The authors show that using their LLM-based dataset, FRet, alone can lead to significant improvement, setting a strong baseline as a zero-shot embedding model on MTEB.