 * The paper proposes Branch-Train-MiX (BTX), a method for training Large Language Models (LLMs) that can specialize in multiple domains such as coding, math reasoning, and world knowledge.
* BTX starts by branching a seed model to train experts in parallel with high throughput and reduced communication cost.
* After individual experts are trained, BTX combines their feedforward parameters as experts in Mixture-of-Expert (MoE) layers and averages the remaining parameters, followed by an MoE-finetuning stage to learn token-level routing.
* BTX generalizes two special cases, the Branch-Train-Merge method and sparse upcycling.
* Compared to alternative approaches, BTX achieves the best accuracy-efficiency tradeoff.
* BTX allows for further supervised finetuning and reinforcement learning from human feedback, unlike the Branch-Train-Merge method.
* The Mixture-of-Experts approach is used to reduce the computational footprint of LLMs, allowing the total number of parameters to grow without additional computation.