 * The paper proposes a method for editing large pre-trained models to correct inaccurate outputs or update outdated information.
* The method, called Model Editor Networks with Gradient Decomposition (MEND), uses a collection of small auxiliary editing networks to make fast, local edits to a pre-trained model's behavior.
* MEND learns to transform the gradient obtained by standard fine-tuning using a low-rank decomposition of the gradient to make the parameterization of this transformation tractable.
* MEND can be trained on a single GPU in less than a day even for 10 billion+ parameter models and enables rapid application of new edits to the pre-trained model.
* The experiments on T5, GPT, BERT, and BART models show that MEND is the only approach to model editing that effectively edits the behavior of models with more than 10 billion parameters.
* The code and data are available at <https://sites.google.com/view/mend-editing>.