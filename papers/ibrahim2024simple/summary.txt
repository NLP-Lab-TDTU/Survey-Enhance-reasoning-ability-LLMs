 * Large language models (LLMs) are typically pre-trained on billions of tokens, and the process is repeated when new data becomes available.
* Continual pre-training of LLMs is more efficient, but the distribution shift in new data can result in degraded performance on previous data or poor adaptation to new data.
* The authors propose a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data to match the performance of fully re-training from scratch.
* The proposed method is shown to match the performance of re-training from scratch on two commonly used LLM pre-training datasets (English→English) and a stronger distribution shift (English→German) at the 405M parameter model scale.
* The method also matches the re-training baseline for a 10B parameter LLM, demonstrating that LLMs can be successfully updated via simple and scalable continual learning strategies, using only a fraction of the compute.
* The authors also propose alternatives to the cosine learning rate schedule to help circumvent forgetting induced by LR re-warming and that are not bound to a fixed token budget.