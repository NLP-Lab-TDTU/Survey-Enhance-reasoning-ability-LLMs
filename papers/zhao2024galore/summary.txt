 * Training Large Language Models (LLMs) is memory-intensive, with optimizer states and gradients taking up more memory than the trainable parameters themselves.
* Current memory-reduction methods, such as low-rank adaptation (LoRA), add a trainable low-rank matrix to the frozen pre-trained weight in each layer, reducing trainable parameters and optimizer states.
* However, these methods typically underperform training with full-rank weights in both pre-training and fine-tuning stages, as they limit the parameter search to a low-rank subspace and alter the training dynamics.
* The authors propose Gradient Low-Rank Projection (GaLore), a memory-efficient training strategy that allows full-parameter learning while reducing memory usage by up to 65.5% in optimizer states.
* GaLore maintains both efficiency and performance for pre-training on LLaMA 1B and 7B architectures with the C4 dataset and fine-tuning RoBERTa on GLUE tasks.
* The authors demonstrate, for the first time, the feasibility of pre-training a 7B model on consumer GPUs with 24GB memory without model parallel, checkpointing, or offloading strategies.