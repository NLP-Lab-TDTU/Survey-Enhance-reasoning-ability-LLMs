 * The paper proposes Reinforced Self-Training (ReST), a simple algorithm for aligning large language models (LLMs) with human preferences using reinforcement learning from human feedback (RLHF).
* ReST is inspired by growing batch reinforcement learning and is more efficient than typical online RLHF methods because it produces the training dataset offline, allowing data reuse.
* The paper focuses on the application of ReST to machine translation and shows that it can substantially improve translation quality, as measured by automated metrics and human evaluation.
* ReST consists of two steps: Grow, where a policy generates a dataset, and Improve, where the filtered dataset is used to fine-tune the policy. These steps are repeated, with Improve step repeated more frequently to amortize the dataset creation cost.
* The authors demonstrate that ReST can align LLMs with human preferences, improving their performance on downstream tasks and reducing the risk of generating unsafe or harmful content.
* ReST is a general approach applicable to all generative learning settings, and the authors suggest that it could be used to improve the performance of LLMs in other domains.