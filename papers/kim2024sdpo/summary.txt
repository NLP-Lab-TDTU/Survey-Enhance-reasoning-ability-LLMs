 * The paper proposes sDPO, an extension of Direct Preference Optimization (DPO) for aligning large language models (LLMs) with human preferences.
* sDPO involves dividing available preference datasets and utilizing them in a step-wise manner during DPO training, allowing for more precise alignment with reference models.
* The authors demonstrate that sDPO results in a final model that is more performant than other LLMs with more parameters.
* DPO involves curating preference datasets using human or strong AI judgement to select chosen and rejected responses to questions, and training LLMs by comparing log probabilities of chosen versus rejected answers.
* However, obtaining these probabilities can be challenging with proprietary models, and the reference model is typically set as the base SFT model, which is a weaker alternative with potentially misaligned preferences.
* sDPO addresses this issue by using the aligned model from the previous step as the reference model for the current step, resulting in a more aligned reference model and a more performant final model.