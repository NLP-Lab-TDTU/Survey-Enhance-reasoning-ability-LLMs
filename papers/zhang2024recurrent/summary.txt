 * The paper proposes an improved approach for speculative decoding to enhance the efficiency of serving large language models (LLMs).
* The method combines the strengths of the classic two-model speculative decoding approach and the single-model approach, Medusa.
* It uses a single-model strategy with a lightweight draft head that has a recurrent dependency design, similar to the small draft model in classic speculative decoding but without the complexities of the full transformer architecture.
* The recurrent dependency allows for swift filtering of undesired candidates using beam search, resulting in a method that is simple in design and avoids creating a data-dependent tree attention structure only for inference, as in Medusa.
* The proposed method is empirically demonstrated to be effective on several popular open-source language models, with a comprehensive analysis of trade-offs involved in adoption.
* Large language models use auto-regressive methods to generate token-by-token responses, and the latency of the single token generation step significantly increases with model size.
* Speculative decoding has emerged as a promising strategy to accelerate LLM inference, using a smaller draft model to generate preliminary candidate tokens more efficiently, followed by verification by the larger target model.
* The single-model approach, as exemplified by Medusa, holds promise for easier integration into existing LLM serving systems.