# References

- [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](papers/wei2023chainofthought/2201.11903.pdf)
- [Yi: Open Foundation Models by 01.AI](papers/ai2024yi/2403.04652.pdf)
- [Fast Model Editing at Scale](papers/mitchell2022fast/2110.11309.pdf)
- [GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection](papers/zhao2024galore/2403.03507.pdf)
- [sDPO: Don't Use Your Data All at Once](papers/kim2024sdpo/2403.19270.pdf)
- [Gecko: Versatile Text Embeddings Distilled from Large Language Models](papers/lee2024gecko/2403.20327.pdf)
- [Advancing LLM Reasoning Generalists with Preference Trees](papers/yuan2024advancing/2404.02078.pdf)
- [Simple and Scalable Strategies to Continually Pre-train Large Language Models](papers/ibrahim2024simple/2403.08763.pdf)
- [ReFT: Representation Finetuning for Language Models](papers/wu2024reft/2404.03592.pdf)
- [RAFT: Adapting Language Model to Domain Specific RAG](papers/zhang2024raft/2403.10131.pdf)
- [OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset](papers/toshniwal2024openmathinstruct1/2402.10176.pdf)
- [Instruction-tuned Language Models are Better Knowledge Learners](papers/jiang2024instructiontuned/2402.12847.pdf)
- [Recurrent Drafter for Fast Speculative Decoding in Large Language Models](papers/zhang2024recurrent/2403.09919.pdf)
- [Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM](papers/sukhbaatar2024branchtrainmix/2403.07816.pdf)
- [InternLM2 Technical Report](papers/cai2024internlm2/2403.17297.pdf)
